{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# negative pair pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pymysql\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get lni-pos-content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_pair = pickle.load(open('/data/yuchen/projects/sentence_embedding/data/pos_pair.pkl','rb'))\n",
    "lni_content = pos_pair[['lni','content']].dropna().groupby('lni')\n",
    "lni_content = lni_content.agg(lambda x: list(x.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get lni-sentances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "lnis = pickle.load(open('/data/yuchen/projects/sentence_embedding/data/lni.pkl','rb'))\n",
    "pos_train_date_2 = pickle.load(open('/data/yuchen/projects/sentence_embedding/data/train/pos_train_date_2.pkl','rb'))\n",
    "pos_train_date_3 = pickle.load(open('/data/yuchen/projects/sentence_embedding/data/train/pos_train_date_3.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "'init mysql connection'\n",
    "db_connection_str = 'mysql+pymysql://shenjiawei:jiaweiDH$z048Kue2*34@cat-cluster.cluster-cvieeiq0uwtk.ap-southeast-1.rds.amazonaws.com:3306/ai_cat_ca_dev'\n",
    "db_connection = create_engine(db_connection_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 296 ms, sys: 48 ms, total: 344 ms\n",
      "Wall time: 4.78 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result=pd.read_sql(keyword_sql, con=db_connection)= \"select l.lni, c.content from lni_unique as l inner join case_sentence as c on l.lni = c.lni where l.lni in \" + '(' + ','.join(list(map(lambda x:'\"' + x + '\"',lni_content.index[:100]))) + ')'\n",
    "result=pd.read_sql(keyword_sql, con=db_connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lni_sentences(lni_content,start,end):\n",
    "    keyword_sql= \"select l.lni, c.content from lni_unique as l inner join case_sentence as c on l.lni = c.lni where l.lni in \" + '(' + ','.join(list(map(lambda x:'\"' + x + '\"',lni_content.index[start:end]))) + ')'\n",
    "    result=pd.read_sql(keyword_sql, con=db_connection)\n",
    "    lni_sentence = result.groupby('lni').agg(list)\n",
    "    return lni_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get neg-sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0308 14:47:17.954418 139890140436224 SentenceTransformer.py:39] Load pretrained SentenceTransformer: /data/yuchen/projects/sentence_embedding/data/model/sts_model\n",
      "I0308 14:47:17.955609 139890140436224 SentenceTransformer.py:100] Load SentenceTransformer from folder: /data/yuchen/projects/sentence_embedding/data/model/sts_model\n",
      "I0308 14:47:19.187472 139890140436224 SentenceTransformer.py:124] Use pytorch device: cpu\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('/data/yuchen/projects/sentence_embedding/data/model/sts_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_rank(gold_sents,all_sents,topk):\n",
    "    #Compute embedding for both lists\n",
    "    embeddings1 = model.encode(gold_sents, convert_to_tensor=True)\n",
    "    embeddings2 = model.encode(all_sents, convert_to_tensor=True)\n",
    "    #Compute cosine-similarits\n",
    "    cosine_scores = util.pytorch_cos_sim(embeddings1, embeddings2)\n",
    "    result = defaultdict(list)\n",
    "    tensor, indice = torch.topk(cosine_scores,topk,largest=False)\n",
    "    unsimilar_sents = []\n",
    "    for i in range(indice.shape[0]):\n",
    "        # print(\"gold_sent{}----------------------------------------------------------------\".format(i))\n",
    "        # print(gold_sents[i])\n",
    "        for j in range(indice.shape[1]):\n",
    "            result[gold_sents[i]].append((all_sents[indice[i,j]],str(cosine_scores[i][indice[i,j]].detach().numpy()))) \n",
    "            unsimilar_sents.append(all_sents[indice[i,j]])\n",
    "        #     print(\"\\n{} \\n Score: {:.4f}\".format(all_sents[indice[i,j]], cosine_scores[i][indice[i,j]]))\n",
    "        # print(\"--------------------------------------------------------------------------\")\n",
    "    return unsimilar_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neg_pair(lni_content,lni_sentance,topk):\n",
    "    neg_train_data = pd.DataFrame(columns=list(POS_TRAIN_DATA))\n",
    "    for index in lni_sentance.index:\n",
    "        # index = '5F16-93C1-DY89-M2CC-00000-00'\n",
    "        paths = POS_TRAIN_DATA[POS_TRAIN_DATA.lni == index].path.unique()\n",
    "        unrelevant_sents = transformer_rank(lni_content.content[index],lni_sentance.content[index],topk)\n",
    "        rows = defaultdict(list)\n",
    "        for p in paths:\n",
    "            for s in unrelevant_sents:\n",
    "                rows['lni'].append(index)\n",
    "                rows['path'].append(p)\n",
    "                rows['content'].append(s)\n",
    "                rows['lable'].append(0)\n",
    "        neg_train_data = neg_train_data.append(pd.DataFrame(rows),ignore_index=True)\n",
    "    return neg_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetching data from mysql\n",
      "fetching data finished\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dad50473ace496ab10f9d006ba37ce0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Batches', max=1, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "261078f04c2247bcbeb305dd8c9517c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Batches', max=3, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64552293bee745bc9d4ec233f42204fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Batches', max=1, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "454bce8a35ae4a0dbef5e1fa8c8b70c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Batches', max=1, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91620b3a4703468684f21f1a8e47ff27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Batches', max=1, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "474b2eb33ac249e696c7482ba9c96df0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Batches', max=8, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d0edfb0b4604403862bc46f58d1a091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Batches', max=1, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2514873384094ba6b2e8e2ec8a77d919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Batches', max=19, style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd14ad25294045898a05e08abf6c0c58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Batches', max=1, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f577204d2ea04f589e9c2b7818b8d4be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Batches', max=33, style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "850e1b3d0ade49d88a88e2a1ff528394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Batches', max=1, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22fe27ac42594c7c9b3ebb98f206cc72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Batches', max=18, style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "697bf59f952545f992f55860371ff092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Batches', max=1, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "425b3729955041c2bc02f45b0df49385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Batches', max=7, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6fd53ff0d614630a7621e3d5500f50b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Batches', max=1, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cad986bef7454d33a269dc345f3c7d74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Batches', max=10, style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b602052034b463fb3d3aa853939d07f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Batches', max=1, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c53070adc392451f81dcb7a28a46524e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Batches', max=4, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2625eae182194bf7991acced681c7a08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Batches', max=1, style=ProgressStyle(description_width='initi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd17dfeda28944d5bcb28db3d7bf027c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Batches', max=11, style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 epochs is finished\n",
      "178 negative samples are created\n",
      "CPU times: user 3min 6s, sys: 4.07 s, total: 3min 10s\n",
      "Wall time: 31.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "step = 200\n",
    "POS_TRAIN_DATA =  pos_train_date_2\n",
    "THRESHHOLD = 2000000\n",
    "\n",
    "start = 0\n",
    "end = step\n",
    "pos_train_data = pd.DataFrame(columns=list(POS_TRAIN_DATA))\n",
    "for epoch in range(len(lni_content.index)//100):\n",
    "    try:\n",
    "        print('fetching data from mysql')\n",
    "        lni_sentance = get_lni_sentences(lni_content,start,end)\n",
    "        print('fetching data finished')\n",
    "    except Exception:\n",
    "        continue\n",
    "    neg_pair = get_neg_pair(lni_content,lni_sentance,2)\n",
    "    pos_train_data = pos_train_data.append(neg_pair,ignore_index=True)\n",
    "    start = start + step\n",
    "    end = end + step\n",
    "    print('{} epochs is finished'.format(epoch+1))\n",
    "    if pos_train_data.shape[0] > THRESHHOLD: \n",
    "        print('{} negative samples are created'.format(pos_train_data.shape[0]))\n",
    "        break\n",
    "pos_train_data.to_pickle('/data/yuchen/projects/sentence_embedding/data/train/neg_train_date_2_test.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_train_data = pickle.load(open('data/train/neg_train_date_2.pkl','rb'))\n",
    "pos_train_data = pickle.load(open('data/train/pos_train_date_2.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = {'criminal law':\n",
    "['5GCD-JP81-DXHD-G3NS-00000-00',\n",
    "'5VB3-JF51-FCYK-20CM-00000-00',\n",
    "'5F16-93C1-FFMK-M0SJ-00000-00',\n",
    "'5F8P-SFG1-JGPY-X039-00000-00']\n",
    ",\n",
    "'family law':\n",
    "['5T3H-Y051-F4GK-M0TD-00000-00',\n",
    "'5J3T-06K1-JYYX-63T2-00000-00']\n",
    ",\n",
    "'civil litigation':\n",
    "['5F16-93D1-JCRC-B25T-00000-00',\n",
    "'5F8P-SFM1-JCBX-S3F2-00000-00']\n",
    ",\n",
    "'immigration law':\n",
    "['5F8W-M4D1-FBFS-S1RS-00000-00']\n",
    ",\n",
    "'damages':\n",
    "['5T04-D4W1-JS0R-2319-00000-00']\n",
    ",\n",
    "'tort law':\n",
    "['5F7T-S7J1-JYYX-62KN-00000-00']}\n",
    "test_cases = [v for k,v in test_cases.items()]\n",
    "test_cases = list(chain(*test_cases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = neg_train_data.iloc[:100,:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_train_data.drop(index=pos_train_data[pos_train_data.lni.apply(lambda x : x in test_cases)].index,inplace=True)\n",
    "neg_train_data.drop(index=neg_train_data[neg_train_data.lni.apply(lambda x : x in test_cases)].index,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## concat pos and neg, shuffle and split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([pos_train_data,neg_train_data])\n",
    "data.to_pickle('data/train/train_date_2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = train_test_split(\n",
    "   data, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## change to sentence transformer input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "from sentence_transformers import SentenceTransformer, LoggingHandler, losses, util, InputExample\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.dropna(inplace=True)\n",
    "valid_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 58s, sys: 476 ms, total: 3min 59s\n",
      "Wall time: 3min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_samples = []\n",
    "valid_samples = []\n",
    "# .iloc[:100000,:]\n",
    "for index, row in train_data.iterrows(): \n",
    "    inp_example = InputExample(texts=[str(row['path']), str(row['content'])], label=float(row['lable']))\n",
    "    train_samples.append(inp_example)\n",
    "pickle.dump(train_samples,open('data/train/final/train_4millon.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in valid_data.iterrows(): \n",
    "    inp_example = InputExample(texts=[str(row['path']), str(row['content'])], label=float(row['lable']))\n",
    "    valid_samples.append(inp_example)\n",
    "pickle.dump(valid_samples,open('data/train/final/valid_4millon.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train\n",
    "training via train.py script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test from path-content pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = '/data/yuchen/projects/sentence_embedding/output/training_stsbenchmark_continue_training-distilbert-base-nli-mean-tokens-2021-03-09_17-56-07'\n",
    "model = SentenceTransformer(model_save_path,device=\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_test(enable_print=False):\n",
    "    index = random.choice(range(valid_data.shape[0]))\n",
    "    sen1,sen2,label = list(valid_data.iloc[index,[1,2,3]])\n",
    "    embedding1 = model.encode([sen1],convert_to_tensor=True)\n",
    "    embedding2 = model.encode([sen2],convert_to_tensor=True)\n",
    "    similarity_score = 1-cosine(embedding1,embedding2)\n",
    "    if enable_print:\n",
    "        print('setence1: {}\\nsentence2: {}\\nlabel: {}\\npredicted: {}\\nerror: {}'.format\\\n",
    "              (sen1,sen2,label,similarity_score,abs(label-similarity_score)))\n",
    "    return label-similarity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0065000541508197784"
      ]
     },
     "execution_count": 616,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors = []\n",
    "for _ in range(100):\n",
    "    errors.append(random_test())\n",
    "max(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test from test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "'init mysql connection'\n",
    "db_connection_str = 'mysql+pymysql://shenjiawei:jiaweiDH$z048Kue2*34@cat-cluster.cluster-cvieeiq0uwtk.ap-southeast-1.rds.amazonaws.com:3306/ai_cat_ca_dev'\n",
    "db_connection = create_engine(db_connection_str)\n",
    "def get_lni_sentences(lnis):\n",
    "    keyword_sql= \"select l.lni, c.content from lni_unique as l inner join case_sentence as c on l.lni = c.lni where l.lni in \" + '(' + ','.join(list(map(lambda x:'\"' + x + '\"',lnis))) + ')'\n",
    "    result=pd.read_sql(keyword_sql, con=db_connection)\n",
    "    lni_sentence = result.groupby('lni').agg(list)\n",
    "    return lni_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_data = pickle.load(open('data/train/neg_train_date_2.pkl','rb'))\n",
    "pos_data = pickle.load(open('data/train/pos_train_date_2.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lni_sens = get_lni_sentences(test_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_rank(gold_sents,all_sents,topk):\n",
    "    #Compute embedding for both lists\n",
    "    embeddings1 = model.encode(gold_sents, convert_to_tensor=True)\n",
    "    embeddings2 = model.encode(all_sents, convert_to_tensor=True)\n",
    "    #Compute cosine-similarits\n",
    "    cosine_scores = util.pytorch_cos_sim(embeddings1, embeddings2)\n",
    "    result = defaultdict(list)\n",
    "    try:\n",
    "        tensor, indice = torch.topk(cosine_scores,topk,largest=True)\n",
    "    except Exception:\n",
    "        print('selected k out of range')\n",
    "        return None\n",
    "    similar_sents = []\n",
    "    for i in range(indice.shape[0]):\n",
    "        # print(\"gold_sent{}----------------------------------------------------------------\".format(i))\n",
    "        # print(gold_sents[i])\n",
    "        for j in range(indice.shape[1]):\n",
    "            result[gold_sents[i]].append((all_sents[indice[i,j]],str(cosine_scores[i][indice[i,j]].detach().numpy()))) \n",
    "            similar_sents.append(all_sents[indice[i,j]])\n",
    "        #     print(\"\\n{} \\n Score: {:.4f}\".format(all_sents[indice[i,j]], cosine_scores[i][indice[i,j]]))\n",
    "        # print(\"--------------------------------------------------------------------------\")\n",
    "    return dict(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5GCD-JP81-DXHD-G3NS-00000-00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Criminal law - Sentencing': [('Sentences to be served consecutively',\n",
       "   '0.99991083'),\n",
       "  ('Commission of offence for criminal organization', '0.99980086'),\n",
       "  ('Possession for purpose of trafficking', '0.99978226')],\n",
       " 'Criminal law - Controlled drugs and substances': [('Possession for purpose of trafficking',\n",
       "   '0.9996006'),\n",
       "  ('Sentences to be served consecutively', '0.99933803'),\n",
       "  ('Commission of offence for criminal organization', '0.9990827')]}"
      ]
     },
     "execution_count": 611,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index= test_cases[0]\n",
    "print(index)\n",
    "all_paths = pos_data[pos_data.lni == index].path.unique()\n",
    "gold_sens = pos_data[pos_data.lni == index].content.unique()\n",
    "all_sens = test_lni_sens.content[index]\n",
    "pre_sens = transformer_rank(all_paths,all_sens,3)\n",
    "# transformer_rank(all_paths,gold_sens,3)\n",
    "pre_sens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
